{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Getting Started with Web Scraping\n",
    "> Web Scraping is basically about extracting data from web pages. \n",
    "\n",
    "> The first step of web scraping is identifying what data points we want and where they are on the webpage. \n",
    "\n",
    ">> For this example we want to scrape the quote, author name, and tags from quotes.toscrape.com/random\n",
    "\n",
    "> We can use scrapy shell to test how to extract the data from our webpage. \n",
    "\n",
    "> In your terminal, or command line, type: \n",
    "\n",
    "**scrapy shell 'quotes.toscrape.com/randomâ€™**\n",
    "\n",
    "> With the command we just passed scrapy will provide us with a **response object** that we can use to access the data from the url we entered. \n",
    "\n",
    "> In your terminal, or command line, type: \n",
    "\n",
    "**view(response)** \n",
    "\n",
    ">will return the page we extracted in a web browser "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**print(response.text)**\n",
    ">will return the entire page context in html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now lets use our **inspect** tool to find what we want to etract on the page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see above that our quote is wrapped in a **span element** with a **text class**.\n",
    "\n",
    "> We will use the response.css method to select which points of the webpage we want to extract. Below is the basic framework we will use to extract data with the scrapy shell.\n",
    "\t\n",
    "**response.css('element.class').extract()**\n",
    "\n",
    "> In your terminal, or command line, type: \n",
    "\n",
    "**response.css('span.text').extract()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We successfuly extracted what we wanted using the last code, but now we want to get rid of the HTML\n",
    "\n",
    "> To get rid of the HTML type:\n",
    "\n",
    "**response.css('span.text::text').extract()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> If we want our data in string format (above is in list format) we can do one of two options:\n",
    "\n",
    "**response.css('span.text::text')[0].extract()**\n",
    "\n",
    "**response.css('span.text::text').extract_first()**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss6.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Finally, we've extracted what we wanted. \n",
    "\n",
    "> Now **you** try extracting the author name (in string format) and the tags (in list format) and in the next section we will create a spider to scrape this data.\n",
    "\n",
    "> When you are finished with the scrapy shell you can type **quit()** to exit the shell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Creating your First Scrapy Spider \n",
    "> Now we will automate the data extraction by building a scrapy spider. In the terminal, or command line, we will type the following command:\n",
    "\n",
    "**scrapy genspider quotes toscrape.com** \n",
    "\n",
    ">The first parameter in the genspider command is 'quotes' - this is the name of our spider. \n",
    "\n",
    ">The second parameter in the genspider command is 'toscrape.com' - this is the url that we will be scraping. \n",
    "\n",
    "<img src=\"img\\ss7.png\">\n",
    "\n",
    ">This command will create a **quotes.py** file in the directory you selected. I didn't change my directory so the file is found in my hoffmanbrandon1 user folder. You can open your file in any type of code editing software. I will be using Atom to manipulate my code. \n",
    "\n",
    ">If you would like, you can find more about Atom here: https://atom.io/\n",
    "\n",
    ">The quotes.py file will look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss8.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can see from this that a spider is merely a python **class** with a few **attributes**:\n",
    "\n",
    ">**name** - name we provided to genspider command \n",
    "\n",
    ">**allowed_dowmains** - we set this attribute when we want to make sure our spider will \tfollow links for certain domains \n",
    "\t\n",
    ">**start_urls** - here we find the urls that our spider will visit when we execute it \n",
    "\n",
    ">For this example, change the **start_urls** attribute to **'http://quotes.toscrape.com/random'**\n",
    "\n",
    ">The most important part of our spider is the parse method. **Scrapy will call this method when responses to the start urls are received.**\n",
    "\n",
    ">We want our spider to create a dictionary with the fields 'author_name', 'text', and 'tags'. We should have found how to scrape these fields in the section 1. All we need to do is put these selectors in a dictionary under the parse method. Your final code for your parse method should look like this: \n",
    "\n",
    "<img src=\"img\\ss9.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Our first spider is finished!\n",
    "\n",
    ">To run our spider we will go to our terminal, or command line, and type:\n",
    "\n",
    "**scrapy runspider quotes.py**\n",
    "\n",
    ">The output will be printed in the terminal. We can save the output to a file by typing:\n",
    "\n",
    "**scrapy runspider quotes.py -o items.json **\n",
    "\t\n",
    ">This will save our output to a file called items.json\n",
    "\n",
    ">To open this file from the terminal we can type:\n",
    "\n",
    "**more items.json**\n",
    "\n",
    "<img src=\"img\\ss10.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Real World Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we will attempt to create a similiar spider that will scrape a job posting from stack overflow.\n",
    "\n",
    ">Follow the link below and use the inspect tool in your browser to check out the webpage. \n",
    "\n",
    ">https://stackoverflow.com/jobs/159909/principal-data-scientist-zalando-se?so=i&pg=1&offset=5\n",
    "\n",
    "<img src=\"img\\ss11.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The spider we want to create will extract the job title, company name, company location, and the perks underneath the company name & location. Because there is the possibility of multiple perks, only one perk, or no perks, we will extract the perks in list format. \n",
    "\n",
    "<img src=\"img\\ss12.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">When we inspect the page we can see that all the information we want is nestled inside a div.-description element. To make the data extraction easier, we will create a variable with this to extract the information from. \n",
    "\n",
    ">Lets get started. In our terminal, or command line, type:\n",
    "\n",
    "**scrapy shell**\n",
    "\n",
    ">After the shell has loaded type:\n",
    "\n",
    "**fetch('url')**\n",
    "\n",
    ">Copy the url from above, or if this job posting has been deleted, use a different job post url. \n",
    "\n",
    ">Now we will create a variable with the div.-description element. \n",
    "\n",
    "**data = response.css('div.-description')**\n",
    "\n",
    ">With this variable we can etract the data inside using data.css() rather than extracting from the entire page with response.css() \n",
    "\n",
    ">Now lets check out the job title to try and extract it. \n",
    "\n",
    "<img src=\"img\\ss13.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Ok so it looks like we want to extract the title attribute from the a element, which is nestled in an h1 element. \n",
    "\n",
    ">In our scrapy shell we'll type: \n",
    "\n",
    "**data.css('h1.-title > a::attr(title)').extract_first()**\n",
    "\n",
    ">Perfect! We got the data we wanted. Now try and do the same for the company name, company location, and perks... Don't feel bad if you cant extract them now. I'll supply the code to extract these, but I want you to notice how they are formatted in the HTML. Then use my code to see how I went about extracting them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Job Title:\n",
    "\n",
    "**data.css('h1.-title > a::attr(title)').extract_first()**\n",
    "\n",
    ">Company Name:\n",
    "\n",
    "**data.css('div.-name > a.employer::text')[0].extract()**\n",
    "\n",
    ">Company Location:\n",
    "\n",
    "**data.css('div.-location::text')[0].extract()**\n",
    "\n",
    ">Perks:\n",
    "\n",
    "**data.css('div.-perks > p::text').extract()**\n",
    "\n",
    ">Now that we have all the data we need from this page we can **quit()** the scrapy shell and create our spider. \n",
    "\n",
    ">In the terminal, or command line, type: \n",
    "\n",
    "**scrapy genspider jobs stackoverflow.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss14.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we just need to change the start_urls attribute to the url we are using, and put our extraction logic under our parse function!\n",
    "\n",
    ">In the end, our spider should look like this: \n",
    "\n",
    "<img src=\"img\\ss15.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now lets run our spider, save the output to json, and view our json file. \n",
    "\n",
    "**scrapy runspider jobs.py -o jobs.json**\n",
    "\n",
    "**more jobs.json**\n",
    "\n",
    "<img src=\"img\\ss16.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We can see from the output that we have successfully extracted \"perks\", \"job_title\", \"company\", and \"location\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Scraping Multiple Items per Page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">For extracting multiple items per page, the best practice to use is extracting all the data from one instance and using a for loop to cycle through the instances. For our example, we will use quotes.toscrape.com. When you go to this site you will see ten quotes on the page. \n",
    "\n",
    ">The data from each quote is nestled inside a div.quote element. Because each quote has the data we need nested in a div.quote element, we will use this in our for loop. \n",
    "\n",
    ">In our scrapy shell we can set the div.quote element to a variable since it nests all the data we want. Then we can find the elements we want to scrape using the variable. (Just like in our last example)\n",
    "\n",
    "<img src=\"img\\ss17.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we will open our scrapy shell and practice extracting the data we want. \n",
    "\n",
    "**scrapy shell quotes.toscrape.com**\n",
    "\n",
    ">Next we will create our variable. We will put [0] at the end of our variable to focus on the first quote, but we will not put it in our spider's for loop.\n",
    "\n",
    "**quote = response.css('div.quote')[0]**\n",
    "\n",
    ">Now our quote variable holds all the data for the first quote. We can use the same extraction logic that we used in our first example, except instead of using response.css() we will use **quote.css()**\n",
    "\n",
    "<img src=\"img\\ss18.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now lets **quit()** our scrapy shell and create a new spider. \n",
    "\n",
    "**scrapy genspider multiple_quotes quotes.toscrape.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss19.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we will put our for loop with our extraction logic in the parse function and our spider will be complete!\n",
    "\n",
    "<img src=\"img\\ss20.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Finally lets run our spider, save the output to json, and view our output.\n",
    "\n",
    "**scrapy runspider multiple_quotes.py -o mquotes.json**\n",
    "\n",
    "**more mquotes.json**\n",
    "\n",
    "<img src=\"img\\ss21.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Real World Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now we're going to extract the same information as we did in our last real world application, except we will extract it from the job listing page. The url we will be using is:\n",
    "\n",
    "https://stackoverflow.com/jobs?med=site-ui&ref=jobs-tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let's start by opening our scrapy shell and fetching this url.\n",
    "\n",
    ">After we fetch the url we can inspect the page and see that each quote is in a div element with a <br> -job-summary class. \n",
    "\n",
    "<img src=\"img\\ss22-1.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We will use this information to create our variable in our shell. \n",
    "\n",
    "**data = response.css('div.-job-summary')[0]**\n",
    "\n",
    ">Now we can further extract the data we want from here using **data.css()** Unfortunately, the extraction logic isn't exactly like the logic we used earlier so we will need to change a couple things. \n",
    "\n",
    ">When we inspect the job title we see that the title is within a h2 element instead of h1 and the class is g-col10. Other than that, the extraction logic remains the same from earlier.\n",
    "\n",
    "<img src=\"img\\ss23.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The extraction logic for the job title is:\n",
    "\n",
    "**data.css('h2.g-col10 > a::attr(title)').extract_first()**\n",
    "\n",
    ">Now try and find the extraction logic to extract company name, location, and perks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Job Title:\n",
    "\n",
    "**data.css('h2.g-col10 > a::attr(title)').extract_first()**\n",
    "\n",
    ">Company Name:\n",
    "\n",
    "**data.css('div.-name::text')[0].extract()**\n",
    "\n",
    ">Company Location:\n",
    "\n",
    "**data.css('div.-location::text')[0].extract()**\n",
    "\n",
    ">Perks:\n",
    "\n",
    "**data.css('div.-perks > span::text').extract()**\n",
    "\n",
    ">Now that we have all the data we need from this page we can **quit()** the scrapy shell and create our spider. \n",
    "\n",
    ">In the terminal, or command line, type: \n",
    "\n",
    "**scrapy genspider multiple_jobs stackoverflow.com**\n",
    "\n",
    ">Now we just need to change the start_urls attribute to the url we are using, and put our extraction logic under our parse function!\n",
    "\n",
    ">In the end, our spider should look like this: \n",
    "\n",
    "<img src=\"img\\ss24.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now let's run our spider but this time lets save it to a csv file. \n",
    "\n",
    "**scrapy runspider multiple_jobs.py -o mjobs.csv**\n",
    "\n",
    ">After the command runs go to the directory that our csv file was saved to and open the file in Excel. The file should look similiar to this: \n",
    "\n",
    "<img src=\"img\\ss25.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Following Pagination Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">In the previous sections, we learned how to extract data from a website. Now we will learn how to crawl a website. Crawling is the ability of our spider to jump from one page to another via hyperlinks. \n",
    "\n",
    ">On our example website quotes.toscrape.com there is a next button that will load ten more quotes. Our goal is to create a spider that will extract all quotes from the domain. The data extraction logic will remain the same from the last section. \n",
    "\n",
    "<img src=\"img\\ss26.png\">\n",
    "\n",
    ">The pagination (next) button is nested in a list element with the class 'next', and inside of it we have an anchor element with the actual link. Because we want the link in the href attribute, our code to extract link from the next button will look like:\n",
    "\n",
    "**response.css('li.next > a::attr(href)').extract_first()**\n",
    "\n",
    ">However, this is a relative url and we need an absolute url. \n",
    "\n",
    "**next_page_url = response.css('li.next > a::attr(href)').extract_first()**\n",
    "\n",
    "**response.urljoin(next_page_url)**\n",
    "\n",
    ">url.join - joins the base url from the **response** to the url that we passed as a **parameter**. \n",
    "\n",
    ">After our spider finds the link to the next page, it needs to create a new request. \n",
    "\n",
    "**yield scrapy.Request(url=next_page_url, callback=self.parse)**\n",
    "\n",
    ">Let's create our spider to scrape all the quotes from this site. \n",
    "\n",
    "**scrapy genspider all_quotes quotes.toscrape.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img\\ss27.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This is how our spider should look in the end. Notice how we put an **if statement** to verify that our spider will stop when there are **no more pages**\n",
    "\n",
    ">Now when we run our spider, we can look at the output in the terminal and see the **item_scraped_count** = 100. Meaning we successfully scraped all ten quotes from all 10 pagination links. \n",
    "\n",
    "<img src=\"img\\ss28.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Real World Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">To finish up our real world spider, we need to inspect the next pagination button and use the same code from above. \n",
    "\n",
    ">Our final spider should look like this:\n",
    "\n",
    "<img src=\"img\\ss29.png\">\n",
    "\n",
    ">Notice the code I put in our if statement. If you delete this code the spider will extract all the jobs from all the pages. I only wanted to get the data from the first two pages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope you enjoyed this tutorial on web scraping with Scrapy. If you would like to learn more on web scraping I would advise you to check out Scrapy's tutorials! \n",
    "\n",
    "https://learn.scrapinghub.com/scrapy/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
